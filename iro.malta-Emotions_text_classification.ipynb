{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b527b25-6feb-4dff-87c4-9502bc13579a",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Deep Learning with Python</h1>\n",
    "<h2>Emotions text classification </h2>\n",
    "<h2>Iro-Georgia Malta</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1996a29-f081-41ba-8ead-72251e327059",
   "metadata": {},
   "source": [
    "<img src=\"https://www.seekpng.com/png/detail/88-884850_inside-joy-sadness-inside-out-pixel-art.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: center; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077a23f-d994-43da-b0ac-88453f440dde",
   "metadata": {},
   "source": [
    "## About the task:\n",
    "The aim of this project is to build a **multi-class classification model** which will be trained on tweets that convey one of the following emotions: joy, sadness, anger or fear. The task is also a single-label classification since each sample requires one label (emotion). The dataset used for this project is the \"Emotion Classification NLP\" which can be found in kaggle (https://www.kaggle.com/datasets/anjaneyatripathi/emotion-classification-nlp?select=emotion-labels-train.csv). Identifying emotions in data (e.g., tweets, articles, reviews etc.) has become an integral part of many NLP and Data Science tasks such as text classification, sentiment analysis or automatic summarization. Additionally, analyzing the emotions expressed in a text can improve the performance of NLP systems predicting the context or the intent of a text. For the reasons mentioned above, I decided to build and train a neural model on this specific dataset.\n",
    "<br>\n",
    "For this project, besides building a multi-class classification model, I will use the **One-vs-Rest** strategy to find which model (mutli-class classification model vs. binary classification model(s)) has higher **accuracy** and **F-measure**, and why this model has better predictions for the dataset. According to the results of the models, some final conlcusions are drawn at the end of the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a95fc-c506-4103-a3bd-c986fd481681",
   "metadata": {},
   "source": [
    "## Import modules and set seed:\n",
    "In this section, I import the modules that are used throughout the whole project. Modules or libraries which have to be imported at a specific point of the project, are not mentioned here. Addionally, I set a seed here in order to reproduce the same results. The function **set_seeds()** must be called before training each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e519cf5-e5bc-4fbe-ab25-2aef90be2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df580ec3-51b9-4679-b755-5ac4dba8140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds():\n",
    "   np.random.seed(123) \n",
    "   python_random.seed(123)\n",
    "   tf.random.set_seed(1234)\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955476a-e6e3-43b2-b77a-7240e7fcb553",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Data Processing:\n",
    "\n",
    "Before I start building the model(s), the dataset needs to be loaded and processed in order to be fed into the neural network(s). To load the dataset and to extract data I use **pandas** library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c788af-892f-4e3d-b93d-0f2dd2eddca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library for extracting data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14a990-e46c-4589-8bdc-bec0ee322692",
   "metadata": {},
   "source": [
    "The dataset are three separate **csv-files**: train, validation and test dataset. The data from the csv-files are loaded and assigned to the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c5c76-3ddb-4ede-a998-f963602b6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_csv() method from pandas library\n",
    "\n",
    "emotions_train_data = pd.read_csv(\"/compLing/students/courses/deepLearning/finalProject23/iro.malta/emotion-labels-train.csv\") # train data\n",
    "emotions_val_data = pd.read_csv(\"/compLing/students/courses/deepLearning/finalProject23/iro.malta/emotion-labels-val.csv\") # validation data\n",
    "emotions_test_data = pd.read_csv(\"/compLing/students/courses/deepLearning/finalProject23/iro.malta/emotion-labels-test.csv\") # test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8183d07-f1a8-4af1-a9a6-3eaf26c5171d",
   "metadata": {},
   "source": [
    "The content of the three different datasets must be visualized in order to know which information needs to be extracted from them. The **head() function** is used and it shows the datasets contain **two columns**: text and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23eb07f-81c2-4498-acf3-8420fc7235c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize train data\n",
    "\n",
    "emotions_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d75fbc-274e-4fce-9903-8fb2c6d67316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize validation data\n",
    "\n",
    "emotions_val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6e283-6afa-49ba-8637-f8da91402493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize test data\n",
    "\n",
    "emotions_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4d216-9518-4671-bb2d-e553e948a6b7",
   "metadata": {},
   "source": [
    "As an extra step, I iterate through the column labels to make sure about the **column labels** (text, label) and **the number of columns** (2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa5ecb-0a38-4bfa-a80c-4e87f6b55376",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in emotions_train_data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27717827-b87b-4fb5-8657-b0342154b844",
   "metadata": {},
   "source": [
    "## 2. Label Encoding:\n",
    "\n",
    "The samples of the three datasets are assigned with the following labels: **joy, sadness, anger and fear**. The format of these labels is not appropriate to be used by a neural network and thus, I convert the labels to classes **0-3**. To encode the labels I use the **preprocessing.LabelEncoder** from **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f83a93-1462-49df-b7c7-d87541b55e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unique elements of the column 'label' in the datasets\n",
    "\n",
    "emotions_train_data['label'].unique() # train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8bda3-1622-4849-bdc6-dcaf22a94eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_val_data['label'].unique() # validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e74cdf-3782-4e5a-995a-7af983510c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_test_data['label'].unique() # test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c5f93-77c9-4710-9f3a-967f711e17fa",
   "metadata": {},
   "source": [
    "All three datasets contain the labels: **joy, fear, anger and sadness**. An additional step of exploring the data of the column 'label' is to **count the instances of each label** in the datasets. For this reason, I use the **value_counts()** function on the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6712ad-58e1-4512-91dc-1d2e7d06c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels_train = emotions_train_data['label'].value_counts()\n",
    "print(count_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd1daf-e6b4-4a03-a231-833505b9e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels_val = emotions_val_data['label'].value_counts()\n",
    "print(count_labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a58b8-f71a-49a8-9aa8-f5510f21670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels_test = emotions_test_data['label'].value_counts()\n",
    "print(count_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44a17e-2a9d-4f5a-849f-99d62b6521c0",
   "metadata": {},
   "source": [
    "The **value_counts()** function returns the instances of each label in a **descending order**. It is observed that the label **'fear'** has the most counts, then the label **'anger'** comes second, and lastly **'joy'** and **'sadness'**. From the label counts, it is also apparent that the train dataset and the test dataset have more samples than the validation dataset.\n",
    "<br>\n",
    "Now, I import the **preprocessing.LabelEncoder** from **scikit-learn** to encode the labels to classes **0-3**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b898c8e-d900-4278-b82b-d9d109faf0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import label encoder from scikit-learn\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c39c5-b54a-475b-85a2-f9b4c264e3b6",
   "metadata": {},
   "source": [
    "I create an extra column with the title **'label_class'** in all three datasets. In this way, I can associate the classes **0-3** with each emotion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01c2e6-860f-4859-8245-d50659298b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder() # assign LabelEncoder object \n",
    "\n",
    "# create column 'label_class' and encode the emotion labels of column 'label'\n",
    "emotions_train_data['label_class'] = label_encoder.fit_transform(emotions_train_data['label']) # train data\n",
    "emotions_val_data['label_class'] = label_encoder.fit_transform(emotions_val_data['label']) # validation data\n",
    "emotions_test_data['label_class'] = label_encoder.fit_transform(emotions_test_data['label']) # test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34e388-d9b1-40a2-8273-5b1a69e6629c",
   "metadata": {},
   "source": [
    "Now, column **'label_class'** has the classes **0-3** as unique elements in all datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba060e8b-daee-46fb-8f87-b6bd02639204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique elements of the column 'label_class'\n",
    "\n",
    "emotions_train_data['label_class'].unique() # train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b684c4-b13b-4163-acf9-6cff44831e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_val_data['label_class'].unique() # validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64ad8e-80fd-4baf-aea2-effb23d4d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_test_data['label_class'].unique() # test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a029d-ee0b-4c4d-b803-4e642c55c060",
   "metadata": {},
   "source": [
    "The extra column **'label_class'** can be found now in the datasets. Here, I check the extra column in the train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd4866-44a4-44a1-bf2b-9ab35de5ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_train_data.head() # train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c193da-0f59-4c11-a15d-d464b94c9965",
   "metadata": {},
   "source": [
    "Now, I use the **value_counts()** function on the three datasets to associate the emotion labels with the label classes. The function returns the associated pairs as well as their counts in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98218a-d396-4329-bf50-22bd447dcc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_train_data[['label', 'label_class']].value_counts() # train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdfbe8-4145-4afe-8908-60cbbbcb2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_val_data[['label', 'label_class']].value_counts() # validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90d3ba-1c11-459b-af38-d52dde3290df",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_test_data[['label', 'label_class']].value_counts() # test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf04ee4-863e-4c79-910d-abd9ecf68819",
   "metadata": {},
   "source": [
    "The associated pairs between the emotions and the classes are: **anger - 0, fear - 1, joy - 2 and sadness - 3**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a33379e-3aff-4480-aa05-6ef01f62de8e",
   "metadata": {},
   "source": [
    "## 3. Conversion of data and labels into numerical formats:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc95a4-4753-4319-bf64-88784dc77ed3",
   "metadata": {},
   "source": [
    "I extract the necessary information from the columns **text** and **label_class** in the three datasets. I put the extracted samples and labels into lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb94be5-bcea-4e6a-8a26-41bd9e2f1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "emotions_train_list = emotions_train_data['text'].tolist()\n",
    "emotions_train_labels = emotions_train_data['label_class'].tolist()\n",
    "\n",
    "# validation data\n",
    "emotions_val_list = emotions_val_data['text'].tolist()\n",
    "emotions_val_labels = emotions_val_data['label_class'].tolist()\n",
    "\n",
    "# test data\n",
    "emotions_test_list = emotions_test_data['text'].tolist()\n",
    "emotions_test_labels = emotions_test_data['label_class'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a381e-2123-4aad-9b61-8f26801928cd",
   "metadata": {},
   "source": [
    "Now, I calculate the **average sentence length** (the mean of sentences length) in the train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcd337-ec19-4f96-a30f-6af07ca524f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean of sentences length in the train data\n",
    "\n",
    "sentence_length = []\n",
    "\n",
    "for l in emotions_train_list:\n",
    "    sentence_length.append(len(l.split(' ')))\n",
    "    \n",
    "sentence_mean = np.mean(sentence_length) # the mean is 16\n",
    "print(sentence_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9aa9e-e3cb-436a-94a2-d298f3198ffe",
   "metadata": {},
   "source": [
    "Additionally, I plot a histogram with **max lengths** of the sentences and **their instences** in the train dataset (counts); I also plot **the mean of sentences length**. For the histogram, **Matplotlib** library is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac90d87-3d91-4701-ba47-665e84a252a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib to visualize max length of sentences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array(sentence_length) # convert the list to numpy array\n",
    "\n",
    "plt.hist(x, color=\"skyblue\", ec=\"white\", lw=1, density=False, bins=20) # density=False for counts\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Max_length')\n",
    "plt.axvline(x.mean(), color='k', linestyle='dashed', linewidth=1) # plot the mean of x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2ad07-80c4-43c2-8296-9c1f4a2b0067",
   "metadata": {},
   "source": [
    "The histogram shows that the **max length** of the setences is around **32**. It also depicts that **the max length of most sentences** can be found around **16** and **24**. Therefore, **the mean of sentences length** is plotted close to **16** which proves that the previous calculation of the mean is correct.\n",
    "<br>\n",
    "Additionally, I calculate **the counts** of the sentences for each max length in the train dataset to see which sentence length has the most sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be772ad-7436-4f7a-a19b-5afe599b9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_count = {}\n",
    "\n",
    "for c in sentence_length:\n",
    "    if c in sentence_count.keys():\n",
    "        sentence_count[c]+=1\n",
    "    else:\n",
    "        sentence_count[c]=1\n",
    "\n",
    "print(sentence_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52612ef-eb7f-43bb-8b2a-78ca6ad887c6",
   "metadata": {},
   "source": [
    "From the counts shown above, most sentences have max length between **16** and **24**. The max length **19** has the most counts of sentences **203**. There is only one sentence of length **58** and thus, it is not depicted in the histogram.\n",
    "<br>\n",
    "I also check sentence lengths normality in the dataset to see if the dataset follows a normal distribution and to determine the max length of sentences for sequence padding more accurately. For this reason, I use **the probability density function** for norm from **scipy.stats**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61f6d6-c29f-4540-89f5-2d02e8131f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x, bins, y = plt.hist(sentence_length, 20, density=True) # density=True for propability density\n",
    "mu = np.mean(sentence_length) # mean\n",
    "sigma = np.std(sentence_length) # SD\n",
    "plt.ylabel('Probability_density')\n",
    "plt.xlabel('Max_length')\n",
    "plt.plot(bins, norm.pdf(bins, mu, sigma)) # plot normal probability density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a22ce7-e564-48d0-93fa-2d691433f9ce",
   "metadata": {},
   "source": [
    "According to the histogram above, it is depicted that there are many sentences with max length higher than **16**. Therefore, truncating the sentences at length **16** might result to losing some valuable information of the sentences. For this reason, I decide to truncate the tweets at **30** words and thus, more data will be included in the training process of the model. \n",
    "<br>\n",
    "Before I start with **sequence padding**, **tokenization** process of the text data takes place first. I only consider the **10.000** most frequent words and thus, I set the parameter *num_words* to *10000* in the **Tokenizer()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da3f5c-23df-4159-ab6e-c032d39a4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tokenizer and padding\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af9f8a-ebdc-47cb-9a5f-0761ed3888dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the tweets after 30 words\n",
    "max_length = 30\n",
    "\n",
    "# consider 10,000 most frequent words for tokenization\n",
    "max_words = 10000\n",
    "\n",
    "# tokenize the datasets and set num_words parameter = max_words\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(emotions_train_list) # train data\n",
    "tokenizer.fit_on_texts(emotions_val_list) # validation data\n",
    "tokenizer.fit_on_texts(emotions_test_list) # test data\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(emotions_train_list) # train data\n",
    "sequences_val = tokenizer.texts_to_sequences(emotions_val_list) # validation data\n",
    "sequences_test = tokenizer.texts_to_sequences(emotions_test_list) # test data\n",
    "\n",
    "# find number of unique tokens\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c6cdb-de93-414c-a5c6-af7dcfdc661c",
   "metadata": {},
   "source": [
    "Now that the sentences from the three datasets are tokenized, the **sequence padding** process can be performed on the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef78b5-a906-46d3-9b3e-0453c018be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences to the same length, max_length = 30\n",
    "train_np_data = pad_sequences(sequences_train, maxlen = max_length) # train data\n",
    "print('Shape of train data tensor:', train_np_data.shape)\n",
    "\n",
    "val_np_data = pad_sequences(sequences_val, maxlen = max_length) # validation data\n",
    "print('Shape of validation data tensor:', val_np_data.shape)\n",
    "\n",
    "test_np_data = pad_sequences(sequences_test, maxlen = max_length) # test data\n",
    "print('Shape of test data tensor:', test_np_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebde78d-e497-441a-a20c-16425c6963d6",
   "metadata": {},
   "source": [
    "The sentences of the datasets are now converted into appropriate **numerical formats**. At this point, I convert the **class labels** into numpy arrays and then, into **one-hot encoding** format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebd7aa-f1b2-4bce-ac19-703932177111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels into numpy arrays and then, into one-hot encoding\n",
    "\n",
    "labels_one_hot_train = np.asarray(emotions_train_labels) # train labels\n",
    "labels_one_hot_train = to_categorical(labels_one_hot_train)\n",
    "print('Shape of label tensor:', labels_one_hot_train.shape)\n",
    "\n",
    "labels_one_hot_val = np.asarray(emotions_val_labels) # validation labels\n",
    "labels_one_hot_val = to_categorical(labels_one_hot_val)\n",
    "print('Shape of label tensor:', labels_one_hot_val.shape)\n",
    "\n",
    "labels_one_hot_test = np.asarray(emotions_test_labels) # test labels\n",
    "labels_one_hot_test = to_categorical(labels_one_hot_test)\n",
    "print('Shape of label tensor:', labels_one_hot_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bd1f8-e4a7-49a0-86f4-3f4d1e442097",
   "metadata": {},
   "source": [
    "For the model's training process, I shuffle the text data of the **train dataset** as well as the corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20abb757-e807-4d17-9132-db295a649aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the train dataset\n",
    "\n",
    "indices = np.arange(train_np_data.shape[0]) \n",
    "np.random.shuffle(indices)\n",
    "train_np_data = train_np_data[indices] # train data\n",
    "labels_one_hot_train = labels_one_hot_train[indices] # train labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6879d0b-e926-4464-9ef1-fee2dcd4b847",
   "metadata": {},
   "source": [
    "Now that the tweets as well as the class labels are converted into appropriate numerical formats, they are assigned to new variables which will be used later for the model's training and testing processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5427278-a252-467e-9028-d5901b6c2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data and labels\n",
    "x_train = train_np_data\n",
    "y_train = labels_one_hot_train\n",
    "\n",
    "# validation data and labels\n",
    "x_val = val_np_data\n",
    "y_val = labels_one_hot_val\n",
    "\n",
    "# test data and labels\n",
    "x_test = test_np_data\n",
    "y_test = labels_one_hot_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f41f8-2c4a-4959-a145-f2b5b89714f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the sum of the splitted data is equal to the sum of all the data\n",
    "\n",
    "len(x_train) + len(x_val) + len(x_test) == len(train_np_data) + len(val_np_data) + len(test_np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb8802-e582-4fb5-8826-54d92e22da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the sum of the splitted labels is equal to the sum of all the labels\n",
    "\n",
    "len(y_train) + len(y_val) + len(y_test) == len(labels_one_hot_train) + len(labels_one_hot_val) + len(labels_one_hot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13e518-86e5-4af5-be25-11a69d05090d",
   "metadata": {},
   "source": [
    "## 4. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d376f-a399-40a6-852e-bc02075cc65c",
   "metadata": {},
   "source": [
    "For the models training process I use the pre-trained GloVe embeddings which are trained on **Twitter data** and they are represented in **200** dimension vectors. The file of the embeddings is **'glove.twitter.27B.200d.txt'** and it can be found in this website (https://nlp.stanford.edu/projects/glove/). \n",
    "<br>\n",
    "First, I read in the file containing the embeddings and then, I pre-process the embeddings so that they can be loaded into the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29fff0-ba1b-4536-9112-c66c946ef064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "glove_path = '/compLing/students/courses/deepLearning/finalProject23/iro.malta/'\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "file_txt = open(os.path.join(glove_path, 'glove.twitter.27B.200d.txt'))\n",
    "for line in file_txt:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "file_txt.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526967b-2e9c-49d5-9e22-90712048a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200 # 200d vectors\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim)) \n",
    "for word, i in word_index.items(): # iterate through the tokens \n",
    "    embedding_vector = embeddings_index.get(word) # return the value of the key\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5c1ba-b604-416d-a154-ac09ebf46628",
   "metadata": {},
   "source": [
    "## 5. Multi-class classification Model Setup, Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9f73b-c7e8-4804-9380-45bbcdee6927",
   "metadata": {},
   "source": [
    "Before I set up the models for training and testing, I import the necessary modules for **model setups, plots, evaluation metrics and confusion matrices**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68596a9-2488-4452-a1eb-4cca288b40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules for building the models, plots, evaluation & confusion matrices\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e0dc0-f989-4f5b-96e1-28f7208b50f2",
   "metadata": {},
   "source": [
    "Additionally, I define the function **plot_confusion_matrix**, which plots a confusion matrix when calling it. The function was found in this website: https://deeplizard.com/learn/video/km7pxKy4UHU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63adfd1b-9914-44e2-a0e6-318d55c592fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to produce a confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168192d-6d73-4819-bb1d-309e6cf5915d",
   "metadata": {},
   "source": [
    "## 5.1. First multi-class classification model:\n",
    "I decide to develop **LSTM models** for this task because they are effective in memorizing important information, and they are capable of learning long-term dependencies, especially in sequence prediction problems. Specifically, I try the following types of **LSTM models**: Vanilla and Stacked LSTM models with/ without dropout rate.\n",
    "\n",
    "## Vanilla LSTM models\n",
    "I set up a **Sequential model** that contains an **Embedding layer** and one hidden **LSTM layer** with **128** units. On the **Embedding layer** the pre-trained embeddings are loaded. The model also contains a **Dense layer** as the output layer, and the layer has **4** output units since the model classifies **4** different types of emotions (anger, fear, joy and sadness). Additionally, I use the **softmax** activation function on the **Dense layer** because the **softmax** function is appropriate for multi-class classification problems with mutually exclusive classes such as this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb24d3b-e6d5-4402-bce0-39c70585674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c0022-8480-405a-8aec-7873ea58726f",
   "metadata": {},
   "source": [
    "I load the GloVe matrix which was prepared into the **Embedding layer**. Additionally, I set the parameter **trainable** to **True** so that the pre-trained embeddings adapt to the specific training set. I also tried setting the parameter **trainable** to **False**, but the model's performance was worse. Thus, in all the models the parameter **trainable** of the pre-trained embeddings is set to **True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd2277-454e-4d79-932f-34891aa37a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc46adb-23e3-4ee0-ac53-6cfb54de3378",
   "metadata": {},
   "source": [
    "For the training process, I use the **RMSProp** algorithm for optimization, and I choose **categoricalCrossentropy** as loss function for the model because the specific task has four labels (e.g., 0, 1, 2, 3). Additionaly, I choose accuracy as metric to monitor the model's performance during training. \n",
    "<br>\n",
    "I fit the model to the training data and the training labels, I train the model over **5** epochs, and I group the data into batches of size **32**. The data for validation are also specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca8b28e-b016-43b0-85e1-a742d50f240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task is a mutli-class classification\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, # training/ fitting the model\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e52ada-82c4-4f5c-9f49-94129257b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7426578-94f1-49e0-b0f8-61d802cccba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting training/ validation accuracy and training/ validation loss\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827645d9-c529-46aa-b132-25c2209aa498",
   "metadata": {},
   "source": [
    "### Summary of results:\n",
    "From the metrics **validation accuracy** and **validation loss**, it is apparent that there is overfitting during training in the validation data after epoch **3**. Thus, I would stop training the model at epoch **3**, where the **validation loss** is the lowest and before the model starts overfitting. In the test data a **~78%** test accuracy is gained; however, the model performs better on the training data (~92%) than on the test data, which signals the presence of overfitting. Thus, I decide to change the parameters in the second model with the aim of improving its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd14ce1-b7b9-4822-83b4-e1d6f0308dd2",
   "metadata": {},
   "source": [
    "## 5.2. Second multi-class classification model: \n",
    "The set up of the second model is similar to the first one: it is a **Sequential model**, containing an **Embedding layer** and one hidden **LSTM layer** with fewer units this time, specifically **64** units. It also contains a **Dense layer** as the output layer, with **4** output units and **softmax** activation function is used on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ee141-5aba-43e5-bcf6-21dbc01aba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "model_2.add(LSTM(64))\n",
    "model_2.add(Dense(4, activation='softmax'))\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb119b-580f-4e4d-a71e-fa7905aa106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe matrix\n",
    "model_2.layers[0].set_weights([embedding_matrix])\n",
    "model_2.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dbdbb-0f03-4279-be23-3c1dd774e6e7",
   "metadata": {},
   "source": [
    "The set up for the model's training is the same as the one of the first model. However, this time I reduce the training **epochs** to **3** because it was previously observed that overfitting occurs after 3 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd64fa-ff75-4a57-ad6c-98fcd3b0a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task is a mutli-class classification\n",
    "              metrics=['acc'])\n",
    "\n",
    "history_2 = model_2.fit(x_train, y_train, # training/ fitting the model\n",
    "                    epochs=3,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276651e7-471a-494e-991c-e5a569fdd18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbecce0f-cb2f-468e-84cc-e9428377d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting training/ validation accuracy and training/ validation loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_2.history['acc']\n",
    "val_acc = history_2.history['val_acc']\n",
    "loss = history_2.history['loss']\n",
    "val_loss = history_2.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4403d9-ad87-419f-848b-439466ba595f",
   "metadata": {},
   "source": [
    "### Summary of results:\n",
    "According to the metrics **validation accuracy** and **validation loss**, this time there is not overfitting during training. However, there is still overfitting in the test dataset: **~75%** test accuracy is gained, while maximum accuracy of **~83%** is gained during training.\n",
    "<br>\n",
    "In comparison to the previous model, the second model has achieved a bit lower **loss** **~66%** in the test dataset. The results of both models are very similar and overall, the models do not perform well when looking at both the **validation loss** and **test loss** of the models. Therefore, I decide to build another model with fewer units in the hidden layer to see if the model will perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a8e56-1546-410d-840e-c3e3149fad79",
   "metadata": {},
   "source": [
    "### 5.3. Third multi-class classification model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab8628-b9ba-441a-a802-6774a468fb9a",
   "metadata": {},
   "source": [
    "The third model's set up is similar to the previous two models; the only difference is the reduced number of the units used in the hidden **LSTM layer**, which is **32** in the third model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26de88f-f7df-426c-b038-314525df25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "model_3.add(LSTM(32))\n",
    "model_3.add(Dense(4, activation='softmax'))\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8236da-2846-4b1b-bb25-f081cd1b575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe matrix\n",
    "model_3.layers[0].set_weights([embedding_matrix])\n",
    "model_3.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be198b-789b-4733-88e3-e936ce24b4db",
   "metadata": {},
   "source": [
    "The model's parameters during training are similar to the previous two models. This time, I train the model for **5** epochs to observe whether overfitting occurs after epoch **3** again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ff761-1b35-4c9f-893c-08b2185e3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task is a mutli-class classification\n",
    "              metrics=['acc'])\n",
    "\n",
    "history_3 = model_3.fit(x_train, y_train, # training/ fitting the model\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471aff1-6a6a-4e5b-84d2-7c3b8a549fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_3.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d8f41c-89eb-4094-9793-cc455cd39194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_3.history['acc']\n",
    "val_acc = history_3.history['val_acc']\n",
    "loss = history_3.history['loss']\n",
    "val_loss = history_3.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20a3ba-df14-4c13-a402-a343e15ff79a",
   "metadata": {},
   "source": [
    "### Summary of results:\n",
    "As it was predicted, overfitting occurs after epoch **3** in the validation dataset. Lower **loss**, around **~62%**, is gained during testing, comparing to the previous model. This could indicate that the reduction of the units in the hidden layer plays a role in the model's performance for this dataset. A maximum accuracy of **78%** is gained in the test dataset, which is again lower than the model's training accuracy **~90%**.\n",
    "<br>\n",
    "I decide to compute other metrics (F-measure, Precision, Recall) and create a confusion matrix with the model's predictions so that I can evaluate the model's performance more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d40b32-0adf-40d3-9f98-240eb62c4b8e",
   "metadata": {},
   "source": [
    "### Confusion Matrix and F-measure, Precision, Recall (model_3):\n",
    "I use **model_3** to make predictions, using the **model_3.predict()**. I store these predictions in **y_pred3** variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351a73b-2c87-4a48-9961-1c8167af9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = model_3.predict(x_test)\n",
    "print(y_pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709c953-b946-420e-a749-4342b351fb05",
   "metadata": {},
   "source": [
    "I use the **argmax() NumPy** function to return the indices of the maximum values along the second axis (axis=1) of the model's predicted probabilities (y_pred3) as well as the true class probabilities (y_test). Then, I create the confusion matrix with those probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12ba86-f074-4576-8d57-513da300fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_3 = np.argmax(y_pred3, axis=1) # converts the predicted probabilities to the predicted class labels\n",
    "y_test_3 = np.argmax(y_test, axis=1) # converts the true class probabilities to the true class labels\n",
    "cm3 = confusion_matrix(y_test_3, y_pred_3) # creates a confusion matrix with y_test and y_pred\n",
    "\n",
    "# use the function to produce the confusion matrix\n",
    "plot_confusion_matrix(cm=cm3, classes=[\"anger\", \"fear\", \"joy\", \"sadness\"], title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b712ce1-b93f-45c5-914e-7c3294e064c0",
   "metadata": {},
   "source": [
    "### Confusion Matrix interpretation (model_3):\n",
    "\n",
    "According to the confusion matrix above (cm3), the model has predicted correctly in total **2.458** out of **3.142** the labels of test data (if the sum of the correctly identified labels is not the same, it means that the values on the confusion matrix changed). The label **fear** is the one with the most correct classifications, then it is label **joy** and lastly, labels **anger** and **sadness**. However, the confusion matrix depicts many missclassifications that the model does. It is apparent that the model missclassifies considerably label **fear** with **sadness** and the opposite. Additionally, label **sadness** seems to be missclassified with **anger**. Furthermore, it is very contradictory that label **joy** is missclassified with **sadness** as well as the other two labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0506e2b-8ea3-486b-8186-b363e14d03ec",
   "metadata": {},
   "source": [
    "I compute the **F1-score**, also known as F-measure. The F1-score reaches a **0.78** value, which is good because it is close to value 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc985143-a731-4731-b598-977d9d95c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(y_test_3, y_pred_3, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8882c-b327-46a6-a3d0-8c549de48eb9",
   "metadata": {},
   "source": [
    "I use the **classification_report()** function to compute **Precision**, **Recall** and **F1-score** of each label class which was predicted by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607725c-e87f-4b35-87fd-960d60a37f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_3 = classification_report(y_test_3, y_pred_3, labels=[0,1,2,3], target_names=[\"anger\", \"fear\", \"joy\", \"sadness\"])\n",
    "print(report_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba28f25-2092-42a2-a4d1-3de4f217ed2b",
   "metadata": {},
   "source": [
    "According to the report above, it is apparent that **anger** and **joy** have the highest precision scores; however, label **anger** (or sadness) has the lowest recall score. Furthemore, **sadness** has the lowest precision score, but a **78%** (or more) recall. Label **joy** has the highest F1-score, while **sadness** has the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0235a2-21f9-4948-ab10-9e36a38744ad",
   "metadata": {},
   "source": [
    "I run the same model again; however, this time I train it for **3** epochs to compare the results of the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab96a43-5e87-4302-a86c-20045e5c499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3b = Sequential()\n",
    "model_3b.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "model_3b.add(LSTM(32))\n",
    "model_3b.add(Dense(4, activation='softmax'))\n",
    "model_3b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2fa5db-6c26-4828-a5b8-b6c07d48ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe matrix\n",
    "model_3b.layers[0].set_weights([embedding_matrix])\n",
    "model_3b.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317290c-9261-4fa8-b8bf-c3898524431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3b.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task is a mutli-class classification\n",
    "              metrics=['acc'])\n",
    "\n",
    "history_3b = model_3b.fit(x_train, y_train, # training/ fitting the model\n",
    "                    epochs=3,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998e6c2-0cb6-47f9-b15f-e112b2a8857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_3b.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4884b-ba73-4bf0-8aa4-3091fbc3070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_3b.history['acc']\n",
    "val_acc = history_3b.history['val_acc']\n",
    "loss = history_3b.history['loss']\n",
    "val_loss = history_3b.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a9e3e-f572-4388-8225-7ebd7b54fb71",
   "metadata": {},
   "source": [
    "### Summary of results:\n",
    "This time there is no overfitting during training. However, a higher loss **~70%** and a lower **accuracy** **~73%** are gained during testing, comparing to **model_3**. There is still overfitting between the maximum accuracy of the test dataset **~73%** and the training dataset **~80%**; however, the difference between the two accuracies is smaller than the difference of **model_3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b8876-2ea4-4c17-80ac-47977a08cd66",
   "metadata": {},
   "source": [
    "### Confusion Matrix and F-measure, Precision, Recall (model_3b):\n",
    "I use **model_3b** to make predictions, using the **model_3b.predict()**. I store these predictions in **y_pred3b** variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07389e-ac16-41ac-b906-1d335ade7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3b = model_3b.predict(x_test)\n",
    "print(y_pred3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf7828-7384-491e-9c03-21bf8cb3a7ff",
   "metadata": {},
   "source": [
    "I use the **argmax() NumPy** function again, to return the indices of the maximum values along the second axis (axis=1). Then, I create the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e7666-4f75-40b7-8c82-ffa51a9cb8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_3b = np.argmax(y_pred3b, axis=1) # converts the predicted probabilities to the predicted class labels\n",
    "y_test_3b = np.argmax(y_test, axis=1) # converts the true class probabilities to the true class labels\n",
    "cm3b = confusion_matrix(y_test_3b, y_pred_3b) # creates a confusion matrix with y_test and y_pred\n",
    "\n",
    "# use the function to produce the confusion matrix\n",
    "plot_confusion_matrix(cm=cm3b, classes=[\"anger\", \"fear\", \"joy\", \"sadness\"], title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c59089-62e4-463b-b614-2cf2899a7505",
   "metadata": {},
   "source": [
    "### Confusion Matrix interpretation (model_3b):\n",
    "\n",
    "According to the confusion matrix above (cm3b), the model has predicted correctly in total **2.335** out of **3.142** the labels of test data (if the sum of the correctly identified labels is not the same, it means that the values on the confusion matrix changed). The label **fear** is the one with the most correct classifications, then it is label **joy** and lastly, labels **sadness** and **anger**. In comparison to **model_3 confusion matrix**, this model has more missclassifications, especially between labels: **anger** and **fear**, **fear** and **sadness** (and the opposite), **joy** and **fear**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76a4d9-40a6-4303-bfe0-d5e1c223204a",
   "metadata": {},
   "source": [
    "I compute the **F1-score**. The F1-score reaches a **~0.73** value, which is lower than the F1-score of **model_3**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6dd9fc-51b0-4ef4-8c6c-a40fd77d1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(y_test_3b, y_pred_3b, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d19780-48c1-4723-9f98-ebceb22669a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_3b = classification_report(y_test_3b, y_pred_3b, labels=[0,1,2,3], target_names=[\"anger\", \"fear\", \"joy\", \"sadness\"])\n",
    "print(report_3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc495c-e1fb-432b-9044-c42d76ea595c",
   "metadata": {},
   "source": [
    "In this report, it is observed that the model's predictions for each label have lower **recall** values and f1-scores than **model_3**. Since the results reported for this model's predictions are worse than the results reported for **model_3**, I reach the conclusion that this model doesn't perform well for this task. Therefore, I decide to build **model_4**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71eba3-c9ac-42f1-abf7-78b8bbdbe977",
   "metadata": {},
   "source": [
    "### 5.4. Fourth multi-class classification model\n",
    "### Vanilla LSTM model with dropout rate = 0.5\n",
    "The taining set up of this model is similar to the previous (three) models; however this time, I set the **dropout** argument of the hidden **LSTM** layer to **0.5**. I also tried other dropout rates, starting from **0.1**, and I found out that the model performs better with **0.5** dropout rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a2134-ad91-468d-abbc-abce4c0d7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Sequential()\n",
    "model_4.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "model_4.add(LSTM(32, dropout=0.5))\n",
    "model_4.add(Dense(4, activation='softmax'))\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7c587-cc80-4852-86f6-2dc327cb8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe matrix\n",
    "model_4.layers[0].set_weights([embedding_matrix])\n",
    "model_4.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b66e7-ef62-4430-87d5-621739db0664",
   "metadata": {},
   "source": [
    "I train the model for **5** epochs and I reduce the batch size of the data from **32** to **16** (I tried also with batch size 32, but the metrics were worse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3b72b-0834-49ef-b475-47103f0787f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task is a mutli-class classification\n",
    "              metrics=['acc'])\n",
    "\n",
    "history_4 = model_4.fit(x_train, y_train, # training/ fitting the model\n",
    "                    epochs=5,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_val, y_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bde2a2-8fa5-4811-85c9-bf96d1dd1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_4.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea99dd6-944b-4ffd-8bf1-8914e9d33d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_4.history['acc']\n",
    "val_acc = history_4.history['val_acc']\n",
    "loss = history_4.history['loss']\n",
    "val_loss = history_4.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4350b9f-681b-4c53-916d-e4161879f226",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary of results:\n",
    "According to the plots above, there is no overfitting during training. I tried training the model for more epochs and I observed that overfitting occurs after epoch 5. Thus, I train the model over **5** epochs. A lower loss **~62%** and a higher accuracy **~79%** is gained during testing, comparing to **model_3**. The difference between the maximum accuracy of the test dataset **~79%** and the training dataset **~80%** is considerably smaller than all the previous models. This is also the case between the loss of the training set **~52%** and the loss of the test set **~62%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b6e4c-43df-4f9e-a949-2777d831856a",
   "metadata": {},
   "source": [
    "### Confusion Matrix and F-measure, Precision, Recall (model_4):\n",
    "I use **model_4** to make predictions, using the **model_4.predict()**. I store these predictions in **y_pred4** variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7c435-3f3a-43bb-9d0a-86fb122f7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = model_4.predict(x_test)\n",
    "print(y_pred4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15fb7f8-a0b6-4e7d-be24-ae2d215252fd",
   "metadata": {},
   "source": [
    "I use the **argmax() NumPy** function again, to return the indices of the maximum values along the second axis (axis=1). Then, I create the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b6f04-b09e-4e3f-9851-af88be450083",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_4 = np.argmax(y_pred4, axis=1)\n",
    "y_test_4 =np.argmax(y_test, axis=1)\n",
    "cm4 = confusion_matrix(y_test_4, y_pred_4)\n",
    "\n",
    "plot_confusion_matrix(cm=cm4, classes=[\"anger\", \"fear\", \"joy\", \"sadness\"], title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47d6d7-e48a-4b58-b6fa-ec6b871eac3b",
   "metadata": {},
   "source": [
    "### Confusion Matrix interpretation (model_4):\n",
    "\n",
    "According to the confusion matrix above (cm4), the model has predicted correctly in total **2.464** out of **3.142** the labels of test data (if the sum of the correctly identified labels is not the same, it means that the values on the confusion matrix changed). The label **fear** is the one with the most correct classifications, then it is label **joy** and lastly, labels **anger** and **sadness**. In comparison to **model_3 confusion matrix**, this model has fewer missclassifications. However, there are still some label pairs which are considerably missclassified such as: **sadness** with **fear** (and the opposite), **anger** with **fear** (and the opposite), **joy** with **fear** (and the opposite), and **anger** with **sadness**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa27c37-c632-4564-9308-8c0bc0417f45",
   "metadata": {},
   "source": [
    "I compute the **F1-score**. The F1-score reaches a **~0.78** value, which is a bit higher than the F1-score of **model_3** and it is closer to value 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a13175-d8fe-44f3-a9fd-e714784e1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(y_test_4, y_pred_4, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd20c7-635d-4c21-8fe1-bdfd0950d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_4 = classification_report(y_test_4, y_pred_4, labels=[0,1,2,3], target_names=[\"anger\", \"fear\", \"joy\", \"sadness\"])\n",
    "print(report_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b482e153-e604-4300-899c-72b3d1f596c3",
   "metadata": {},
   "source": [
    "Label **fear** and **sadness** have lower precision values than the other two labels. Label **joy** (or anger) has the highest precision value, and label **fear** has the highest recall value. Looking at the results reported, it can be concluded that this is the best model for this task so far because the f1-scores of each class are greater than the ones from **model_3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03224361-d443-479e-8fcd-af7db6243324",
   "metadata": {},
   "source": [
    "### 5.5. Fifth multi-class classification model\n",
    "### Stacked LSTM model with dropout rate = 0.4\n",
    "I decide to build a **Stacked LSTM** model including a dropout rate in the hidden layers to see whether the accuracy of the model will be improved. I set up a **Sequential model**, containing an **Embedding layer** and two hidden **LSTM layers** with **32** units. In the first **LSTM** hidden layer, I set the argument **return_sequences** to **True**, and in both hidden layers I set the dropout rate to **0.4** (I also tried with lower dropout rates, but the model performed worse). The output layer, which is a **Dense layer**, has the same contents as the previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e9900-dbcd-4452-b8fb-01969bb2bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Sequential()\n",
    "model_5.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "model_5.add(LSTM(32, return_sequences=True, dropout=0.4))\n",
    "model_5.add(LSTM(32, dropout=0.4))\n",
    "model_5.add(Dense(4, activation='softmax'))\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19545d4-fb94-4fda-8b8a-a6d2d3676ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe matrix\n",
    "model_5.layers[0].set_weights([embedding_matrix])\n",
    "model_5.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e7e4a-2028-4110-8a91-fa1a966613f0",
   "metadata": {},
   "source": [
    "For the model's training process, I reduce the batche size to **16** and I train the model for **7** epochs (I tried also with 10 epochs, but overfitting occurs after epoch 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788a91c-e060-4877-a3ca-c236b500da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task is a mutli-class classification\n",
    "              metrics=['acc'])\n",
    "\n",
    "history_5 = model_5.fit(x_train, y_train, # training/ fitting the model\n",
    "                    epochs=7,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_val, y_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b215b5a-6832-4c0b-987e-9b6c577d0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_5.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716792d7-307c-43c1-b1d3-0d93e0d2e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_5.history['acc']\n",
    "val_acc = history_5.history['val_acc']\n",
    "loss = history_5.history['loss']\n",
    "val_loss = history_5.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd34aa3-70ea-4214-972e-f962ed7bc685",
   "metadata": {},
   "source": [
    "### Summary of results:\n",
    "The **Stacked LSTM model_5 with dropout rate 0.4** performs better than the previous models, model_4 and model_3. It achieves maximum **validation accuracy** of **~79%** as well as a **~61%** for validation loss. Overfitting is reduced during training and thus, maximum accuracy of **~81%** with **~54%** loss is gained during testing. The difference between training and testing maximum accuracy is also smaller, **~88%** training and **~81%** test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48505bcb-0179-4d3c-a5d6-f2e4092897a6",
   "metadata": {},
   "source": [
    "### Confusion Matrix and F-measure, Precision, Recall (model_5):\n",
    "I use **model_5** to make predictions, using the **model_5.predict()**. I store these predictions in **y_pred5** variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c518a7-89a0-462c-be40-57c6abee9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred5 = model_5.predict(x_test)\n",
    "print(y_pred5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c50ea-7165-4cd5-ad8e-99667f7a7ecd",
   "metadata": {},
   "source": [
    "I use the **argmax() NumPy** function again, to return the indices of the maximum values along the second axis (axis=1). Then, I create the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2a57f-b454-45dc-ae32-f2c57d2205fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_5 = np.argmax(y_pred5, axis=1)\n",
    "y_test_5 =np.argmax(y_test, axis=1)\n",
    "cm5 = confusion_matrix(y_test_5, y_pred_5)\n",
    "\n",
    "plot_confusion_matrix(cm=cm5, classes=[\"anger\", \"fear\", \"joy\", \"sadness\"], title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6264f1f-8947-4865-b1fc-1a7e5d667dcd",
   "metadata": {},
   "source": [
    "### Confusion Matrix interpretation (model_5):\n",
    "\n",
    "According to the confusion matrix above (cm5), the model has predicted correctly in total **2.560** out of **3.142** the labels of test data (if the sum of the correctly identified labels is not the same, it means that the values on the confusion matrix changed). The label **fear** is the one with the most correct classifications, then it is label **anger** and lastly, labels **joy** and **sadness**. In comparison to previous confusion matrices, this model has fewer missclassifications. However, there are still some label pairs which are considerably missclassified such as: **fear** with **sadness** (and the opposite), **fear** with **anger** (and the opposite), **joy** with **fear**, **joy** with **sadness**, and **sadness** with **anger**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca118173-d015-4060-b2d2-d59a39e3d5c8",
   "metadata": {},
   "source": [
    "I compute the **F1-score**. The F1-score reaches a **~0.81** value, which is higher from all the previous F1-scores and it is closer to value 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e7bbc-a293-471f-ade4-b5e842e02b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(y_test_5, y_pred_5, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00400486-e17d-4630-8fde-0d9fd3172a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_5 = classification_report(y_test_5, y_pred_5, labels=[0,1,2,3], target_names=[\"anger\", \"fear\", \"joy\", \"sadness\"])\n",
    "print(report_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df5f4b-771b-41f4-ba48-9cd9c0eccebf",
   "metadata": {},
   "source": [
    "The classification report demonstrates high precision, recall and f1-score values for each class, which indicates that this is a better and more suited model for this task than **model_4**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e35418-291d-4752-9497-240699f93390",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.6. Sixth multi-class classification model\n",
    "### Stacked LSTM model with dropout rate = 0.5\n",
    "I build another **Stacked LSTM** model including a dropout rate in the hidden layers to see whether the accuracy of the model will be improved. Similarly to the previous model, I set up a **Sequential model**, containing an **Embedding layer** and two hidden **LSTM layers** with **32** units. However, this time both hidden layers have dropout rate of **0.5**. The output layer, which is a **Dense layer**, has the same contents as the previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dcb8df-dcfa-4824-9da1-7036d00e88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6 = Sequential()\n",
    "model_6.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "model_6.add(LSTM(32, return_sequences=True, dropout=0.5))\n",
    "model_6.add(LSTM(32, dropout=0.5))\n",
    "model_6.add(Dense(4, activation='softmax'))\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c1087-69e7-4b92-820f-e1e035782d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe matrix\n",
    "model_6.layers[0].set_weights([embedding_matrix])\n",
    "model_6.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66400cc1-e9a8-47ff-b540-abc3d75520fb",
   "metadata": {},
   "source": [
    "I train again the model over **7 epochs** to see whether **model_6** can gain similar or better results than **model_5** (I also tried training the model more than 7 epochs, but overfitting after 7 epochs occurs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a1826-dd37-48a6-9ed0-d37eed4907b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task is a mutli-class classification\n",
    "              metrics=['acc'])\n",
    "\n",
    "history_6 = model_6.fit(x_train, y_train, # training/ fitting the model\n",
    "                    epochs=7,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_val, y_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724ef5b-c5cb-4515-bec0-038522991c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_6.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dec4f2-d19b-4a48-9848-f44acec40a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_6.history['acc']\n",
    "val_acc = history_6.history['val_acc']\n",
    "loss = history_6.history['loss']\n",
    "val_loss = history_6.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4763a-f948-45b5-8f48-dc3919783166",
   "metadata": {},
   "source": [
    "### Summary of results:\n",
    "The **Stacked LSTM model_6 with dropout rate 0.5** seems to have similar performance to **model_5**. It achieves maximum **validation accuracy** of **~76%** as well as a **~63%** for validation loss. A maximum accuracy of **~80%** with **~55%** loss is gained during testing. The difference between training and testing maximum accuracy is also smaller, **~82%** training and **~80%** test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985c5af-445c-4083-8801-2ae64bc478be",
   "metadata": {},
   "source": [
    "### Confusion Matrix and F-measure, Precision, Recall (model_6):\n",
    "I use **model_6** to make predictions, using the **model_6.predict()**. I store these predictions in **y_pred6** variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb54518-8c7f-4537-8422-761673ba7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred6 = model_6.predict(x_test)\n",
    "print(y_pred6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef640f9-b020-485e-b8b3-bffe3e0ae5c2",
   "metadata": {},
   "source": [
    "I use the **argmax() NumPy** function again, to return the indices of the maximum values along the second axis (axis=1). Then, I create the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ef553-a640-4f13-9b85-ef0378c7835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_6 = np.argmax(y_pred6, axis=1)\n",
    "y_test_6 =np.argmax(y_test, axis=1)\n",
    "cm6 = confusion_matrix(y_test_6, y_pred_6)\n",
    "\n",
    "plot_confusion_matrix(cm=cm6, classes=[\"anger\", \"fear\", \"joy\", \"sadness\"], title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc5747-9bdb-4705-8eb1-5778863d2f74",
   "metadata": {},
   "source": [
    "### Confusion Matrix interpretation (model_6):\n",
    "\n",
    "According to the confusion matrix above (cm6), the model has predicted correctly in total **2.512** out of **3.142** the labels of test data (if the sum of the correctly identified labels is not the same, it means that the values on the confusion matrix changed). The label **fear** is the one with the most correct classifications, then it is label **anger** and lastly, labels **joy** and **sadness**. In comparison to the previous confusion matrix of model_5, the model has more missclassifications, but their results are similar. However, there are still some label pairs which are considerably missclassified such as: **fear** with **anger** (and the opposite),**sadness** with **anger**, **sadness** with **fear**, and **joy** with **anger** as well as with **fear**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad972ad-4fa8-49f9-a873-a24b5e557cea",
   "metadata": {},
   "source": [
    "I compute the **F1-score**. The F1-score reaches a **0.80** value, which is close to the F1-score of model_5 (0.81):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b1a2f-c0de-4243-95c1-a73bfc61245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(y_test_6, y_pred_6, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f61669-1c54-4fb6-acab-113d2edf153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_6 = classification_report(y_test_6, y_pred_6, labels=[0,1,2,3], target_names=[\"anger\", \"fear\", \"joy\", \"sadness\"])\n",
    "print(report_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a474b-ac1e-479d-b0f6-116e9aae774d",
   "metadata": {},
   "source": [
    "The classification report demonstrates high precision, recall and f1-score values for each class. The results reported are similar to the ones of **model_5**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7cd2c-7794-4089-8879-96c345f399d9",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "**Model_5** is **the best multi-class classification model** for this task with **0.81** for F1-score. **Model_6** comes in second place with **0.80** for F1-score. **Model_4** could be considered as a third option, but its F1-score **0.78** is lower than the F1-score of the other two models.\n",
    "<br>\n",
    "Now, I will apply **model_5** with One vs. Rest strategy to see if model_5 performs good in a binary classification problem with the same data. Additionally, I will find out with which class (e.g., anger, fear, joy, sadness) the model gains higher accuracy and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49867cca-062e-4679-a9c2-81e21a21117d",
   "metadata": {},
   "source": [
    "## 6. One vs. Rest strategy\n",
    "The One-vs-Rest strategy splits a multi-class classification into one binary classification problem per class. Given the multi-class classification problem with examples for each class **anger, fear, joy and sadness**, this can be divided into **four** binary classification problems as follows:\n",
    "\n",
    "* **Binary classification problem 1**: anger vs [fear, joy, sadness]\n",
    "* **Binary classification problem 2**: fear vs [anger, joy, sadness]\n",
    "* **Binary classification problem 3**: joy vs [anger, fear, sadness]\n",
    "* **Binary classification problem 4**: sadness vs [anger, fear, joy]\n",
    "\n",
    "Therefore, one model must be created for each problem (or class) mentioned above. Before I start building the models for each class, I prepare the labels of the twitter data accordingly. In the three datasets (train, validation, test), I create four extra columns. Each column will contain the new labels for each binary classification problem and these labels will be used by the binary classification models to be trained and tested.\n",
    "\n",
    "## 6.1. Data processing (One vs. Rest)\n",
    "\n",
    "I create the function **add_ovr_label()**, which takes as argument a **Pandas DataFrame** (in this case the train, validation and test datasets) and creates the following **four** columns: **'ovr_for_label_anger', 'ovr_for_label_fear', 'ovr_for_label_joy', 'ovr_for_label_sadness'**. In these columns, the class mentioned in the name of the column (e.g., anger in 'ovr_for_label_anger') is substituted with **1**, and the other classes are substituted with **0** by the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d1165-3944-4855-87db-d634f8302ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ovr_label(data: pd.DataFrame):\n",
    "    data['ovr_for_label_anger'] = data['label'].apply(lambda x: 1 if x == 'anger' else 0)\n",
    "    data['ovr_for_label_fear'] = data['label'].apply(lambda x: 1 if x == 'fear' else 0)\n",
    "    data['ovr_for_label_joy'] = data['label'].apply(lambda x: 1 if x == 'joy' else 0)\n",
    "    data['ovr_for_label_sadness'] = data['label'].apply(lambda x: 1 if x == 'sadness' else 0)\n",
    "\n",
    "\n",
    "# call the function to create four columns with new labels\n",
    "add_ovr_label(emotions_train_data) # train data\n",
    "add_ovr_label(emotions_val_data) # validation data\n",
    "add_ovr_label(emotions_test_data) # test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489046e-110a-4da8-96c4-1d20eceebc0b",
   "metadata": {},
   "source": [
    "I check that the four columns have been added to the three datasets using the **head()** and **value.counts()** functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc76746-943c-426b-8dcc-33d68bbb1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize train data\n",
    "emotions_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678ff15-8492-4bfb-81d4-23265f8eae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize validation data\n",
    "emotions_val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8af51-716f-402a-b64c-4ca6c7febb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize test data\n",
    "emotions_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd3a5f-a087-4148-953c-b139591d4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_train_data[['label', 'ovr_for_label_anger', 'ovr_for_label_fear', 'ovr_for_label_joy',\n",
    "                     'ovr_for_label_sadness']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c937a-450d-4c77-b2a0-639229fcbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_val_data[['label', 'ovr_for_label_anger', 'ovr_for_label_fear', 'ovr_for_label_joy',\n",
    "                     'ovr_for_label_sadness']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909cd092-9553-42f7-8137-0cfc0803387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_test_data[['label', 'ovr_for_label_anger', 'ovr_for_label_fear', 'ovr_for_label_joy',\n",
    "                     'ovr_for_label_sadness']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567ec44-a91d-444f-af3e-cc964530117a",
   "metadata": {},
   "source": [
    "The **value_counts()** function shows above that each class for each binary classification problem is substituted with **1** in the columns, while the other classes are substituted with **0**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836d2e4-5394-4d50-9845-358fe1923021",
   "metadata": {},
   "source": [
    "## 6.3. Vectorizing labels:\n",
    "The text data are already converted into apropriate numerical formats. Now, it remains to vectorize the new binary labels for each classification problem. First, I put the extracted labels into lists and then, I vectorize the labels by encoding them into one-hot format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb785f-8b70-4d50-bbc4-03c5276c0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train labels\n",
    "y_train_for_label_anger = emotions_train_data['ovr_for_label_anger'].tolist()\n",
    "y_train_for_label_fear = emotions_train_data['ovr_for_label_fear'].tolist()\n",
    "y_train_for_label_joy = emotions_train_data['ovr_for_label_joy'].tolist()\n",
    "y_train_for_label_sadness = emotions_train_data['ovr_for_label_joy'].tolist()\n",
    "\n",
    "# validation labels\n",
    "y_val_for_label_anger = emotions_val_data['ovr_for_label_anger'].tolist()\n",
    "y_val_for_label_fear = emotions_val_data['ovr_for_label_fear'].tolist()\n",
    "y_val_for_label_joy = emotions_val_data['ovr_for_label_joy'].tolist()\n",
    "y_val_for_label_sadness = emotions_val_data['ovr_for_label_joy'].tolist()\n",
    "\n",
    "# test labels\n",
    "y_test_for_label_anger = emotions_test_data['ovr_for_label_anger'].tolist()\n",
    "y_test_for_label_fear = emotions_test_data['ovr_for_label_fear'].tolist()\n",
    "y_test_for_label_joy = emotions_test_data['ovr_for_label_joy'].tolist()\n",
    "y_test_for_label_sadness = emotions_test_data['ovr_for_label_joy'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baceb1f7-6579-4546-8c99-811405dd0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification problem 1: anger vs [fear, joy, sadness]\n",
    "\n",
    "y_one_hot_anger_train = np.asarray(y_train_for_label_anger) # train labels\n",
    "y_one_hot_anger_train = to_categorical(y_one_hot_anger_train)\n",
    "print('Shape of label tensor for anger:', y_one_hot_anger_train.shape)\n",
    "\n",
    "y_one_hot_anger_val = np.asarray(y_val_for_label_anger) # validation labels\n",
    "y_one_hot_anger_val = to_categorical(y_one_hot_anger_val)\n",
    "print('Shape of label tensor for anger:', y_one_hot_anger_val.shape)\n",
    "\n",
    "y_one_hot_anger_test = np.asarray(y_test_for_label_anger) # test labels\n",
    "y_one_hot_anger_test = to_categorical(y_one_hot_anger_test)\n",
    "print('Shape of label tensor for anger:', y_one_hot_anger_test.shape)\n",
    "\n",
    "\n",
    "# Binary classification problem 2: fear vs [anger, joy, sadness]\n",
    "\n",
    "y_one_hot_fear_train = np.asarray(y_train_for_label_fear) # train labels\n",
    "y_one_hot_fear_train = to_categorical(y_one_hot_fear_train)\n",
    "print('Shape of label tensor for fear:', y_one_hot_anger_train.shape)\n",
    "\n",
    "y_one_hot_fear_val = np.asarray(y_val_for_label_fear) # validation labels\n",
    "y_one_hot_fear_val = to_categorical(y_one_hot_fear_val)\n",
    "print('Shape of label tensor for fear:', y_one_hot_fear_val.shape)\n",
    "\n",
    "y_one_hot_fear_test = np.asarray(y_test_for_label_fear) # test labels\n",
    "y_one_hot_fear_test = to_categorical(y_one_hot_fear_test)\n",
    "print('Shape of label tensor for fear:', y_one_hot_fear_test.shape)\n",
    "\n",
    "\n",
    "# Binary classification problem 3: joy vs [anger, fear, sadness]\n",
    "\n",
    "y_one_hot_joy_train = np.asarray(y_train_for_label_joy) # train labels\n",
    "y_one_hot_joy_train = to_categorical(y_one_hot_joy_train)\n",
    "print('Shape of label tensor for joy:', y_one_hot_joy_train.shape)\n",
    "\n",
    "y_one_hot_joy_val = np.asarray(y_val_for_label_joy) # validation labels\n",
    "y_one_hot_joy_val = to_categorical(y_one_hot_joy_val)\n",
    "print('Shape of label tensor foy joy:', y_one_hot_joy_val.shape)\n",
    "\n",
    "y_one_hot_joy_test = np.asarray(y_test_for_label_joy) # test labels\n",
    "y_one_hot_joy_test = to_categorical(y_one_hot_joy_test)\n",
    "print('Shape of label tensor for joy:', y_one_hot_joy_test.shape)\n",
    "\n",
    "\n",
    "# Binary classification problem 4: sadness vs [anger, fear, joy]\n",
    "\n",
    "y_one_hot_sadness_train = np.asarray(y_train_for_label_sadness) # train labels\n",
    "y_one_hot_sadness_train = to_categorical(y_one_hot_sadness_train)\n",
    "print('Shape of label tensor for sadness:', y_one_hot_sadness_train.shape)\n",
    "\n",
    "y_one_hot_sadness_val = np.asarray(y_val_for_label_sadness) # validation labels\n",
    "y_one_hot_sadness_val = to_categorical(y_one_hot_sadness_val)\n",
    "print('Shape of label tensor for sadness:', y_one_hot_sadness_val.shape)\n",
    "\n",
    "y_one_hot_sadness_test = np.asarray(y_test_for_label_sadness) # test labels\n",
    "y_one_hot_sadness_test = to_categorical(y_one_hot_sadness_test)\n",
    "print('Shape of label tensor for sadness:', y_one_hot_sadness_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ac34d-bc79-4171-a707-15a13e768a43",
   "metadata": {},
   "source": [
    "## 6.4. Binary classification Models Setup, Training and Testing:\n",
    "### 6.4.1 Binary classification problem 1: anger vs [fear, joy, sadness]\n",
    "I set up the exact same **model_5** with stacked LSTM hidden layers, which each layer contains **32 units** and dropout rate **0.4**. As the output layer I set again a Dense layer, but this time with **2** units because the task has two labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf856da-ebf1-4581-8d45-dbb0cb0fefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_1 = Sequential()\n",
    "binary_model_1.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "binary_model_1.add(LSTM(32, return_sequences=True, dropout=0.4))\n",
    "binary_model_1.add(LSTM(32, dropout=0.4))\n",
    "binary_model_1.add(Dense(2, activation='softmax'))\n",
    "binary_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14a882-d1e2-4fc8-9165-a7fa56f3f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe matrix\n",
    "binary_model_1.layers[0].set_weights([embedding_matrix])\n",
    "binary_model_1.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613fb51-0900-4cf7-8be3-b9fae046e799",
   "metadata": {},
   "source": [
    "I train the model over **7 epochs**, as I did in the multi-class classification task for **model_5**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf6c09-6743-4d97-b606-eb448c142f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_1.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task has two labels\n",
    "              metrics=['acc'])\n",
    "\n",
    "binary_history_1 = binary_model_1.fit(x_train, y_one_hot_anger_train, # training/ fitting the model\n",
    "                    epochs=7,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_val, y_one_hot_anger_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258a66f-ebd2-47b0-a8b7-ea91e2393511",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = binary_model_1.evaluate(x_test, y_one_hot_anger_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6db4f-0ea6-4068-bab6-01c8b103fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = binary_history_1.history['acc']\n",
    "val_acc = binary_history_1.history['val_acc']\n",
    "loss = binary_history_1.history['loss']\n",
    "val_loss = binary_history_1.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998f4a8-8106-4272-aa6b-1be6478fb5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary1 = binary_model_1.predict(x_test)\n",
    "print(y_pred_binary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467da7ac-eeb7-42a6-905d-d513c54019fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary_1 = np.argmax(y_pred_binary1, axis=1)\n",
    "y_one_hot_anger_test_1 = np.argmax(y_one_hot_anger_test, axis=1)\n",
    "cm1_binary = confusion_matrix(y_one_hot_anger_test_1, y_pred_binary_1)\n",
    "\n",
    "plot_confusion_matrix(cm=cm1_binary, classes=[\"anger\", \"Rest\"], title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c8d9d-a233-45d1-a5bb-9a77cb7f8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(y_one_hot_anger_test_1, y_pred_binary_1, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205eefb1-293a-4ceb-8503-4a62bea58394",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_binary_1 = classification_report(y_one_hot_anger_test_1, y_pred_binary_1, labels=[1,0], target_names=[\"anger\", \"Rest\"])\n",
    "print(report_binary_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b5c45-b0f3-495b-a840-032843cdd5f5",
   "metadata": {},
   "source": [
    "### Summary of binary_model_1 results:\n",
    "The results overall are bad, when trying to classify label **anger** vs the rest of the other labels. According to the confusion matrix, it is apparent that the model underperforms a lot in comparison to **model_5** since it struggles to distinguish **anger** from the rest of the labels. For this reason, a very low F1-score **0.46** is gained from this model. The results could be improved maybe by changing some parameters of the model (e.g., reducing the dropout rate to 0.2); however, it does fall out of scope for this project's goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2b05f-e647-4238-a08d-7a093fbbcf04",
   "metadata": {},
   "source": [
    "### 6.4.2 Binary classification problem 2: fear vs [anger, joy, sadness]\n",
    "Similar to the previous binary classification model, I set up the exact same **model_5** for the binary classification problem, fear vs the rest of the labels, and I train the model for **7** epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594bec2-a3dc-4066-b0fc-7d541a1891ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2 = Sequential()\n",
    "binary_model_2.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "binary_model_2.add(LSTM(32, return_sequences=True, dropout=0.4))\n",
    "binary_model_2.add(LSTM(32, dropout=0.4))\n",
    "binary_model_2.add(Dense(2, activation='softmax'))\n",
    "binary_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73ded0-f795-4ff6-8058-d8ab080d983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe matrix\n",
    "binary_model_2.layers[0].set_weights([embedding_matrix])\n",
    "binary_model_2.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1aaf6-d919-462a-ac22-425c187e4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_2.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task has two labels\n",
    "              metrics=['acc'])\n",
    "\n",
    "binary_history_2 = binary_model_2.fit(x_train, y_one_hot_fear_train, # training/ fitting the model\n",
    "                    epochs=7,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_val, y_one_hot_fear_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d341ac9-15d4-49a1-8582-244ed4b9ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = binary_model_2.evaluate(x_test, y_one_hot_fear_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077ac60-007c-4ebc-8324-321ebd029905",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary2 = binary_model_2.predict(x_test)\n",
    "print(y_pred_binary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce8a6a-d774-4251-b9c6-4bc458ae99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary_2 = np.argmax(y_pred_binary2, axis=1)\n",
    "y_one_hot_fear_test_2 = np.argmax(y_one_hot_fear_test, axis=1)\n",
    "cm2_binary = confusion_matrix(y_one_hot_fear_test_2, y_pred_binary_2)\n",
    "\n",
    "plot_confusion_matrix(cm=cm2_binary, classes=[\"fear\", \"Rest\"], title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c4bc0-042b-41ff-b4fe-38b6c108a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(y_one_hot_fear_test_2, y_pred_binary_2, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ef8cc-6d62-4900-8763-8872c1fe841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_binary_2 = classification_report(y_one_hot_fear_test_2, y_pred_binary_2, labels=[1,0], target_names=[\"fear\", \"Rest\"])\n",
    "print(report_binary_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb99572-02c4-4f0c-b6b6-e8d85497de75",
   "metadata": {},
   "source": [
    "### Summary of binary_model_2 results:\n",
    "Similar to the previous binary model, the results are bad again, when binary_model_2 classifies label **anger** vs the rest of the other labels. According to the confusion matrix, it is apparent that the model underperforms a lot in comparison to **model_5** since it struggles to distinguish **fear** from the rest of the labels. For this reason, a very low F1-score **0.45** is gained from this model, which is a bit more than from the previous binary model. The reason behind the higher F1-score **0.12** for label **fear** could be the fact that label **fear** has more samples (995) available than the other labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9808936-dc83-46f4-a1be-f59a2ad1f9f8",
   "metadata": {},
   "source": [
    "### 6.4.3 Binary classification problem 3: joy vs [anger, fear, sadness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8fa1f-9df8-498e-b94a-a4da586cebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3 = Sequential()\n",
    "binary_model_3.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "binary_model_3.add(LSTM(32, return_sequences=True, dropout=0.4))\n",
    "binary_model_3.add(LSTM(32, dropout=0.4))\n",
    "binary_model_3.add(Dense(2, activation='softmax'))\n",
    "binary_model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc4882-81ec-4a26-adbc-7f3bde57fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe matrix\n",
    "binary_model_3.layers[0].set_weights([embedding_matrix])\n",
    "binary_model_3.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d6a7c-9012-4aac-b8df-ab160cabd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_3.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task has two labels\n",
    "              metrics=['acc'])\n",
    "\n",
    "binary_history_3 = binary_model_3.fit(x_train, y_one_hot_joy_train, # training/ fitting the model\n",
    "                    epochs=7,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_val, y_one_hot_joy_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd6dfb-616b-452a-ad0c-7d80807ba0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = binary_model_3.evaluate(x_test, y_one_hot_joy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02fe8f-660e-4a98-98ba-64551178ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary3 = binary_model_3.predict(x_test)\n",
    "print(y_pred_binary3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b723c87-4df3-4c57-bb1a-8828ac1cb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary_3 = np.argmax(y_pred_binary3, axis=1)\n",
    "y_one_hot_joy_test_3 = np.argmax(y_one_hot_joy_test, axis=1)\n",
    "cm3_binary = confusion_matrix(y_one_hot_joy_test_3, y_pred_binary_3)\n",
    "\n",
    "plot_confusion_matrix(cm=cm3_binary, classes=[\"joy\", \"Rest\"], title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea1b26-2b82-4018-950a-d1384d0d6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(y_one_hot_joy_test_3, y_pred_binary_3, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b06ad0-7e1e-40f5-ab2d-987758d31577",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_binary_3 = classification_report(y_one_hot_joy_test_3, y_pred_binary_3, labels=[1,0], target_names=[\"joy\", \"Rest\"])\n",
    "print(report_binary_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec97b3-d8e0-4df8-9651-213e59320944",
   "metadata": {},
   "source": [
    "### Summary of binary_model_3 results:\n",
    "Similar to the previous binary models, the results are bad, when binary_model_3 classifies label **joy** vs the rest of the other labels. According to the confusion matrix, it is apparent that the model underperforms a lot in comparison to **model_5** since it struggles to distinguish **joy** from the rest of the labels. For this reason, a very low F1-score **0.44** is gained from this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475b1de-eea9-40c2-8ac3-c53455f3aa5d",
   "metadata": {},
   "source": [
    "### 6.4.4 Binary classification problem 4: sadness vs [anger, fear, joy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32450de3-38d7-490a-9d0a-0c0c8d8b546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_4 = Sequential()\n",
    "binary_model_4.add(Embedding(max_words, embedding_dim, input_length = max_length))\n",
    "binary_model_4.add(LSTM(32, return_sequences=True, dropout=0.4))\n",
    "binary_model_4.add(LSTM(32, dropout=0.4))\n",
    "binary_model_4.add(Dense(2, activation='softmax'))\n",
    "binary_model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc26fc2-2f84-4f64-854b-ec449578eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe matrix\n",
    "binary_model_4.layers[0].set_weights([embedding_matrix])\n",
    "binary_model_4.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6119f89-0b1e-4884-a231-05921a09e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_4.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', # the task has two labels\n",
    "              metrics=['acc'])\n",
    "\n",
    "binary_history_4 = binary_model_4.fit(x_train, y_one_hot_sadness_train, # training/ fitting the model\n",
    "                    epochs=7,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_val, y_one_hot_sadness_val), # specify validation data\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0c904-35a5-4bcb-9fc8-11190002911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = binary_model_4.evaluate(x_test, y_one_hot_sadness_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf76e9-5300-4107-868c-86ddc265207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary4 = binary_model_4.predict(x_test)\n",
    "print(y_pred_binary4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da54bb-9680-4224-b868-bdc98deea0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary_4 = np.argmax(y_pred_binary4, axis=1)\n",
    "y_one_hot_sadness_test_4 = np.argmax(y_one_hot_sadness_test, axis=1)\n",
    "cm4_binary = confusion_matrix(y_one_hot_sadness_test_4, y_pred_binary_4)\n",
    "\n",
    "plot_confusion_matrix(cm=cm4_binary, classes=[\"sadness\", \"Rest\"], title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196e24f-624b-4fd5-84ab-cbc9250ae522",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(y_one_hot_sadness_test_4, y_pred_binary_4, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8bf2a-addc-4653-a8c7-28992f6ab302",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_binary_4 = classification_report(y_one_hot_sadness_test_4, y_pred_binary_4, labels=[1,0], target_names=[\"sadness\", \"Rest\"])\n",
    "print(report_binary_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd387e5-e7f6-4dcf-99a4-bc9fba09950d",
   "metadata": {},
   "source": [
    "### Summary of binary_model_4 results:\n",
    "Similar to the previous binary models, the results are bad, when binary_model_4 classifies label **sadness** vs the rest of the other labels. According to the confusion matrix, it is apparent that the model underperforms a lot in comparison to **model_5** since it struggles to distinguish **sadness** from the rest of the labels. For this reason, a very low F1-score **0.44** is gained from this model. Labels **sandess** has also the lowest precision score in comparison to the other labels of the previous binary models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688dc50-fdc1-4868-bd25-664f46b6dca8",
   "metadata": {},
   "source": [
    "## Final conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c90cb4-31e1-4161-bbc3-a1a91f981e80",
   "metadata": {},
   "source": [
    "For the performance of the models on the task of text classification according to the emotions, **anger, fear, joy and sadness**, I draw the following conclusions:\n",
    "1. The multi-class classification model(s) performed better on the task than the binary classification models. This can be concluded from the F1-scores of the models as well as from the confusion matrices. **Model_5** is the best model for this task because it gains **0.81** for F1-score.\n",
    "2. Since all the binary classification models struggle to correctly classify one of the emotion labels from the rest, it means that the specific task is too complicated to be solved by a binary classification model. However, the binary classification models could gain better accuracy and F1-scores if the parameters of **model_5** were adjusted differently.\n",
    "3. The reason behind the large number of missclassifications in the **binary classification models** could be the nature and the size of the dataset:\n",
    "- The dataset contains some unuseful information, such as hashtags, other users mentions (@...), (maybe emojis) etc., which creates extra noise during training the model. As a next step, the dataset could be preprocessed in order to improve the model's performance.\n",
    "- The amount of samples for each label is unbalanced for the binary classification models. This can be observed with the **value_counts()** and the **classification_report()**, where **Rest** has always more samples than the specific label. In general though, label **fear** has the most samples and therefore, it always has the most correct predictions from the multi-class models.\n",
    "4. The samples of labels **anger, fear and sadness** must be re-evaluated because they might not be representative enough for each class and therefore, they are missclassified a lot with each other. This issue could also be solved with more labeled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
